{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, regularizers, callbacks\n",
    "from keras.api.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# %%\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"./lucas_pre.csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify column types\n",
    "target = \"pH_H2O\"\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target in numeric_cols:\n",
    "    numeric_cols.remove(target)\n",
    "\n",
    "categorical_cols = [\"Depth\", \"LC\", \"LU\", \"USDA\", \"ISSS\", \"NUTS_0\", \"LC0_Desc\", \"LC1_Desc\", \"LU1_Desc\"]\n",
    "# Filter to only keep categorical columns that exist in the dataset\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "# Remove columns with too many unique values or too many missing values\n",
    "filtered_cat_cols = []\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        if df[col].nunique() < 30 and df[col].isna().sum() / len(df) < 0.3:\n",
    "            filtered_cat_cols.append(col)\n",
    "\n",
    "print(f\"Using {len(numeric_cols)} numeric columns and {len(filtered_cat_cols)} categorical columns\")\n",
    "\n",
    "# Drop rows where target is missing\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "# Feature engineering - create new features\n",
    "if \"EC\" in df.columns and \"OC\" in df.columns:\n",
    "    df[\"EC_OC_ratio\"] = df[\"EC\"] / df[\"OC\"].replace(0, 0.001)\n",
    "\n",
    "if \"Clay\" in df.columns and \"Sand\" in df.columns:\n",
    "    df[\"Clay_Sand_ratio\"] = df[\"Clay\"] / df[\"Sand\"].replace(0, 0.001)\n",
    "\n",
    "if \"N\" in df.columns and \"P\" in df.columns and \"K\" in df.columns:\n",
    "    # NPK balance is important for soil chemistry\n",
    "    if df[\"N\"].notnull().sum() > 0 and df[\"P\"].notnull().sum() > 0 and df[\"K\"].notnull().sum() > 0:\n",
    "        df[\"NPK_sum\"] = df[\"N\"] + df[\"P\"] + df[\"K\"]\n",
    "\n",
    "if \"CaCO3\" in df.columns:\n",
    "    # Soil pH is strongly related to CaCO3\n",
    "    df[\"log_CaCO3\"] = np.log1p(df[\"CaCO3\"])\n",
    "\n",
    "if \"OC\" in df.columns and \"N\" in df.columns:\n",
    "    # C:N ratio is important for soil biology\n",
    "    df[\"CN_ratio\"] = df[\"OC\"] / df[\"N\"].replace(0, 0.001)\n",
    "\n",
    "if \"Clay\" in df.columns and \"OC\" in df.columns:\n",
    "    # Clay-organic matter interactions affect pH\n",
    "    df[\"Clay_OC_interaction\"] = df[\"Clay\"] * df[\"OC\"]\n",
    "\n",
    "# Extract target values\n",
    "y = df[target].values\n",
    "\n",
    "# Update numeric columns after adding engineered features\n",
    "numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns.tolist() if col != target]\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", KNNImputer(n_neighbors=5)),\n",
    "        (\"power\", PowerTransformer(method=\"yeo-johnson\")),  # Better than StandardScaler for skewed data\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "\n",
    "# Combine all transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, numeric_cols), (\"cat\", categorical_transformer, filtered_cat_cols)]\n",
    ")\n",
    "\n",
    "# Prepare features\n",
    "X = df[numeric_cols + filtered_cat_cols]\n",
    "\n",
    "# Split data before preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed feature shape: {X_train_processed.shape}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "def create_cnn_model(\n",
    "    input_dim,\n",
    "    neurons_layers=[128, 64, 32],\n",
    "    dropout_rates=[0.4, 0.3, 0.2],\n",
    "    activation=\"relu\",\n",
    "    learning_rate=0.001,\n",
    "    l2_reg=0.001,\n",
    "    cnn_filters=32,\n",
    "    kernel_size=3,\n",
    "):\n",
    "    # For CNN, we need to reshape the input data\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Reshape layer: transform tabular data to 1D sequence for CNN\n",
    "    # Input shape: (input_dim,) -> Reshape to (input_dim, 1)\n",
    "    model.add(layers.Reshape((input_dim, 1), input_shape=(input_dim,)))\n",
    "\n",
    "    # 1D CNN layer\n",
    "    model.add(layers.Conv1D(filters=cnn_filters, kernel_size=kernel_size, padding=\"same\", activation=activation))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Add a second CNN layer to learn more complex patterns\n",
    "    model.add(layers.Conv1D(filters=cnn_filters * 2, kernel_size=kernel_size, padding=\"same\", activation=activation))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Global pooling layer to convert features back to tabular format\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # Hidden dense layers\n",
    "    for i, neurons in enumerate(neurons_layers):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                neurons,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                kernel_initializer=\"he_normal\",\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropout_rates[i]))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the CNN model\n",
    "model = create_cnn_model(\n",
    "    input_dim=X_train_processed.shape[1],\n",
    "    neurons_layers=[256, 128, 64, 32],\n",
    "    dropout_rates=[0.5, 0.4, 0.3, 0.2],\n",
    "    activation=\"elu\",\n",
    "    learning_rate=0.0005,\n",
    "    l2_reg=0.0005,\n",
    "    cnn_filters=32,\n",
    "    kernel_size=3,\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test_processed, y_test)\n",
    "print(f\"Test Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test_processed)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"mae\"])\n",
    "plt.plot(history.history[\"val_mae\"])\n",
    "plt.title(\"Model MAE\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "plt.xlabel(\"Actual pH\")\n",
    "plt.ylabel(\"Predicted pH\")\n",
    "plt.title(f\"Actual vs Predicted pH (R² = {r2:.4f})\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
