{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, regularizers, callbacks\n",
    "from keras.api.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# %%\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"./lucas_pre.csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify column types\n",
    "target = \"pH_H2O\"\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target in numeric_cols:\n",
    "    numeric_cols.remove(target)\n",
    "\n",
    "categorical_cols = [\"Depth\", \"LC\", \"LU\", \"USDA\", \"ISSS\", \"NUTS_0\", \"LC0_Desc\", \"LC1_Desc\", \"LU1_Desc\"]\n",
    "# Filter to only keep categorical columns that exist in the dataset\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "# Remove columns with too many unique values or too many missing values\n",
    "filtered_cat_cols = []\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        if df[col].nunique() < 30 and df[col].isna().sum() / len(df) < 0.3:\n",
    "            filtered_cat_cols.append(col)\n",
    "\n",
    "print(f\"Using {len(numeric_cols)} numeric columns and {len(filtered_cat_cols)} categorical columns\")\n",
    "\n",
    "# Drop rows where target is missing\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "# Feature engineering - create new features\n",
    "if \"EC\" in df.columns and \"OC\" in df.columns:\n",
    "    df[\"EC_OC_ratio\"] = df[\"EC\"] / df[\"OC\"].replace(0, 0.001)\n",
    "\n",
    "if \"Clay\" in df.columns and \"Sand\" in df.columns:\n",
    "    df[\"Clay_Sand_ratio\"] = df[\"Clay\"] / df[\"Sand\"].replace(0, 0.001)\n",
    "\n",
    "if \"N\" in df.columns and \"P\" in df.columns and \"K\" in df.columns:\n",
    "    # NPK balance is important for soil chemistry\n",
    "    if df[\"N\"].notnull().sum() > 0 and df[\"P\"].notnull().sum() > 0 and df[\"K\"].notnull().sum() > 0:\n",
    "        df[\"NPK_sum\"] = df[\"N\"] + df[\"P\"] + df[\"K\"]\n",
    "\n",
    "if \"CaCO3\" in df.columns:\n",
    "    # Soil pH is strongly related to CaCO3\n",
    "    df[\"log_CaCO3\"] = np.log1p(df[\"CaCO3\"])\n",
    "\n",
    "if \"OC\" in df.columns and \"N\" in df.columns:\n",
    "    # C:N ratio is important for soil biology\n",
    "    df[\"CN_ratio\"] = df[\"OC\"] / df[\"N\"].replace(0, 0.001)\n",
    "\n",
    "if \"Clay\" in df.columns and \"OC\" in df.columns:\n",
    "    # Clay-organic matter interactions affect pH\n",
    "    df[\"Clay_OC_interaction\"] = df[\"Clay\"] * df[\"OC\"]\n",
    "\n",
    "# Extract target values\n",
    "y = df[target].values\n",
    "\n",
    "# Update numeric columns after adding engineered features\n",
    "numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns.tolist() if col != target]\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", KNNImputer(n_neighbors=5)),\n",
    "        (\"power\", PowerTransformer(method=\"yeo-johnson\")),  # Better than StandardScaler for skewed data\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "\n",
    "# Combine all transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, numeric_cols), (\"cat\", categorical_transformer, filtered_cat_cols)]\n",
    ")\n",
    "\n",
    "# Prepare features\n",
    "X = df[numeric_cols + filtered_cat_cols]\n",
    "\n",
    "# Split data before preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed feature shape: {X_train_processed.shape}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# Define a function to create the neural network model\n",
    "def create_model(\n",
    "    input_dim,\n",
    "    neurons_layers=[128, 64, 32],\n",
    "    dropout_rates=[0.4, 0.3, 0.2],\n",
    "    activation=\"relu\",\n",
    "    learning_rate=0.001,\n",
    "    l2_reg=0.001,\n",
    "):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "\n",
    "    # Hidden layers\n",
    "    for i, neurons in enumerate(neurons_layers):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                neurons,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                kernel_initializer=\"he_normal\",\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropout_rates[i]))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# %%\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    neurons_layers = []\n",
    "    dropout_rates = []\n",
    "\n",
    "    # Define ranges for each layer's neurons and dropout\n",
    "    for i in range(n_layers):\n",
    "        neurons_layers.append(trial.suggest_int(f\"n_units_l{i}\", 16, 512))\n",
    "        dropout_rates.append(trial.suggest_float(f\"dropout_l{i}\", 0.1, 0.5))\n",
    "\n",
    "    # Other hyperparameters\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"elu\", \"selu\"])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    l2_reg = trial.suggest_float(\"l2_reg\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "\n",
    "    # Create model with the suggested hyperparameters\n",
    "    model = create_model(\n",
    "        input_dim=X_train_processed.shape[1],\n",
    "        neurons_layers=neurons_layers,\n",
    "        dropout_rates=dropout_rates,\n",
    "        activation=activation,\n",
    "        learning_rate=learning_rate,\n",
    "        l2_reg=l2_reg,\n",
    "    )\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=0)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_processed,\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,  # Reduced max epochs for optimization\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0,  # Silent training for optimization\n",
    "    )\n",
    "\n",
    "    # Get validation MAE from the best epoch\n",
    "    val_mae = min(history.history[\"val_mae\"])\n",
    "\n",
    "    return val_mae  # Return validation MAE to minimize\n",
    "\n",
    "\n",
    "# %%\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)  # Adjust number of trials as needed\n",
    "\n",
    "# %%\n",
    "# Print optimization results\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value (Validation MAE): {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Create and train the model with the best hyperparameters\n",
    "best_n_layers = trial.params[\"n_layers\"]\n",
    "best_neurons = [trial.params[f\"n_units_l{i}\"] for i in range(best_n_layers)]\n",
    "best_dropouts = [trial.params[f\"dropout_l{i}\"] for i in range(best_n_layers)]\n",
    "\n",
    "best_model = create_model(\n",
    "    input_dim=X_train_processed.shape[1],\n",
    "    neurons_layers=best_neurons,\n",
    "    dropout_rates=best_dropouts,\n",
    "    activation=trial.params[\"activation\"],\n",
    "    learning_rate=trial.params[\"learning_rate\"],\n",
    "    l2_reg=trial.params[\"l2_reg\"],\n",
    ")\n",
    "\n",
    "# Print best model summary\n",
    "best_model.summary()\n",
    "\n",
    "# %%\n",
    "# Train the best model with the optimal hyperparameters\n",
    "early_stopping = callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=trial.params[\"batch_size\"],\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Evaluate the best model\n",
    "loss, mae = best_model.evaluate(X_test_processed, y_test)\n",
    "print(f\"Test Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_model.predict(X_test_processed)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"mae\"])\n",
    "plt.plot(history.history[\"val_mae\"])\n",
    "plt.title(\"Model MAE\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Visualize optimization process\n",
    "plt.figure(figsize=(10, 6))\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot parameter importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot parallel coordinate plot for hyperparameters\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_parallel_coordinate(study)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "plt.xlabel(\"Actual pH\")\n",
    "plt.ylabel(\"Predicted pH\")\n",
    "plt.title(f\"Actual vs Predicted pH (R² = {r2:.4f})\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
